# PyTorch å¤š GPU åŒæ—¶è®­ç»ƒæ“ä½œæŒ‡å—

> ðŸ§  é¢å‘ **ç¬¬ä¸€æ¬¡å°è¯•å¤š GPU å¹¶è¡Œè®­ç»ƒ** çš„å®Œæ•´æ•™ç¨‹  
> âœ… å¸®åŠ©ä½ ç†è§£æ¯ä¸€æ­¥åŽŸç†ï¼Œå¹¶èƒ½è½»æ¾è¿ç§»åˆ°è‡ªå·±çš„å…¶ä»–é¡¹ç›®

---

## ðŸ“ é¡¹ç›®ç»“æž„
```text
â”œâ”€â”€ Multi-GPU-simultaneous-training-operation-readme/   # æºç ä¸Žç¤ºä¾‹ä»£ç å®žçŽ°
â”œâ”€â”€ result_informer02_random10000GPU2/                  # å®Œæ•´æ•°æ®é›†è¿è¡Œç»“æžœï¼ˆæ—¥å¿— / æ¨¡åž‹ / å›¾è¡¨ï¼‰
â”œâ”€â”€ Multi-GPU-Training.png                              # å¤š GPU å·¥ä½œçŠ¶æ€æˆªå›¾
â”œâ”€â”€ datatry.csv                                         # ç¤ºä¾‹æ•°æ®é›†ï¼ˆ10,000 è¡Œ Ã— 12 åˆ—ï¼‰
â”œâ”€â”€ informer.py                                         # Minimal Informer å¤š GPU è®­ç»ƒè„šæœ¬
â””â”€â”€ README.md                                           # æœ¬è¯´æ˜Žæ–‡ä»¶
```

---

## ðŸ§± ç›®å½•

1. [èƒŒæ™¯çŸ¥è¯†](#èƒŒæ™¯çŸ¥è¯†)
2. [å¿«é€Ÿå¼€å§‹ï¼š5 ä¸ªå…³é”®æ­¥éª¤](#å¿«é€Ÿå¼€å§‹5ä¸ªå…³é”®æ­¥éª¤)
3. [é€šç”¨æ¨¡æ¿](#é€šç”¨æ¨¡æ¿)
4. [å¸¸è§é”™è¯¯ä¸ŽæŽ’æŸ¥](#å¸¸è§é”™è¯¯ä¸ŽæŽ’æŸ¥)
5. [è®­ç»ƒå£è¯€](#è®­ç»ƒå£è¯€)

---

## ðŸ§  èƒŒæ™¯çŸ¥è¯†

- åœ¨ **PyTorch** ä¸­ï¼Œæ¨¡åž‹é»˜è®¤è¿è¡Œåœ¨å•ä¸ª GPUï¼ˆä¾‹å¦‚ `cuda:0`ï¼‰
- å¦‚æžœæœ‰å¤šä¸ª GPUï¼Œå¯**å¹¶è¡Œå¤„ç†æ•°æ®**ä»¥æå‡è®­ç»ƒé€Ÿåº¦
- PyTorch æä¾›ä¸¤ç§å¤šå¡è®­ç»ƒæ–¹å¼ï¼š
  1. **`nn.DataParallel`**ï¼šå•è¿›ç¨‹å¤šçº¿ç¨‹ï¼Œæœ€æ˜“ä¸Šæ‰‹ï¼Œé€‚åˆä¸­ç­‰è§„æ¨¡è®­ç»ƒ
  2. **`torch.distributed` / `torchrun`**ï¼šå¤šè¿›ç¨‹é«˜æ•ˆå¹¶è¡Œï¼Œé€‚åˆå¤§åž‹é›†ç¾¤è®­ç»ƒ

> æœ¬æ–‡ä½¿ç”¨ `nn.DataParallel`ï¼Œæ›´æ˜“ç†è§£ã€‚å¦‚éœ€æ›´ä¼˜æ€§èƒ½ï¼Œå¯è¿›ä¸€æ­¥è¿ç§»åˆ° `DistributedDataParallel`

---

## ðŸš€ å¿«é€Ÿå¼€å§‹ï¼š5 ä¸ªå…³é”®æ­¥éª¤

> ä»¥ä¸‹ç¤ºä¾‹å‡è®¾å·²æœ‰æ¨¡åž‹ `MyModel` å’Œæ•°æ®è¿­ä»£å™¨ `train_loader` / `test_loader`

### 1ï¸âƒ£ è®¾å®šä¸»è®¾å¤‡ä¸º `cuda:0`
```python
import torch
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
```

### 2ï¸âƒ£ å°è£…æ¨¡åž‹ä¸ºå¤š GPU æ¨¡åž‹
```python
from torch import nn

model = MyModel()
if torch.cuda.device_count() > 1:
    print(f"Using {torch.cuda.device_count()} GPUs â€¦")
    model = nn.DataParallel(model)   # å¤šå¡å°è£…
model = model.to(device)
```

### 3ï¸âƒ£ è®­ç»ƒé˜¶æ®µï¼šæ•°æ®æ‰¹ä¹Ÿè¦ `.to(device)`
```python
for xb, yb in train_loader:
    xb, yb = xb.to(device), yb.to(device)
    out = model(xb)  # DataParallel è‡ªåŠ¨åˆ†å‘æ•°æ®å’Œæ”¶é›†æ¢¯åº¦
    # loss, backward, step â€¦
```

### 4ï¸âƒ£ éªŒè¯ / æŽ¨ç†é˜¶æ®µåŒæ ·è¿ç§»åˆ° GPU
```python
model.eval()
with torch.no_grad():
    for xb in test_loader:
        xb   = xb.to(device)
        pred = model(xb)
        # è¯„ä¼° â€¦
```

### 5ï¸âƒ£ å¯é€‰ï¼šæ‰“å° GPU çŠ¶æ€ä¿¡æ¯
```python
print(f"Available GPUs: {torch.cuda.device_count()}")
for i in range(torch.cuda.device_count()):
    print(f"GPU {i}: {torch.cuda.get_device_name(i)}")
```

---

## ðŸ§© é€šç”¨æ¨¡æ¿

```python
import torch
from torch import nn

# è®¾å®šä¸»è®¾å¤‡
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# æž„å»ºæ¨¡åž‹
model = MyModel()
if torch.cuda.device_count() > 1:
    print(f"Using {torch.cuda.device_count()} GPUs â€¦")
    model = nn.DataParallel(model)
model = model.to(device)

# è®­ç»ƒå¾ªçŽ¯
for xb, yb in dataloader:
    xb, yb = xb.to(device), yb.to(device)
    pred   = model(xb)
    # loss, backward, optimizer.step() â€¦
```

---

## â— å¸¸è§é”™è¯¯ä¸ŽæŽ’æŸ¥

| æŠ¥é”™ / çŽ°è±¡ | å¯èƒ½åŽŸå›  | å¿«é€Ÿè§£å†³æ–¹æ¡ˆ |
|-------------|-----------|----------------|
| `RuntimeError: module must have its parameters on cuda:0 but got cuda:1` | æ¨¡åž‹æœªåœ¨ä¸»è®¾å¤‡ä¸Š | å°è£…å®Œå†è°ƒç”¨ `.to(device)` |
| `Expected all tensors to be on the same device` | è¾“å…¥æ•°æ®æˆ–æ ‡ç­¾æœª `.to()` | æ‰€æœ‰è¾“å…¥æ•°æ®éƒ½æ‰§è¡Œ `.to(device)` |
| å¤š GPU æœªæé€Ÿ | æ¨¡åž‹å¤ªå° / batch å¤ªå° | å°è¯•å¢žå¤§ `batch_size` æˆ–æ¢ç”¨ DDP |
| `AttributeError: 'DataParallel' object has no attribute 'xxx'` | è®¿é—®äº†å°è£…åŽçš„æ¨¡åž‹å±žæ€§ | ç”¨ `model.module.xxx` æ¥è®¿é—®å†…éƒ¨å±žæ€§ |

---

## ðŸ§  è®­ç»ƒå£è¯€ï¼ˆè®°å¿†å…¬å¼ï¼‰

> å¤šå¡è®­ç»ƒäº”è¦ç´ ï¼Œå£è¯€å¦‚ä¸‹ï¼š

```text
è®¾ä¸»å¡ â†’ device = cuda:0  
å°æ¨¡åž‹ â†’ nn.DataParallel(model)  
æ”¾ä¸»å¡ â†’ model.to(device)  
æ•°æ®è½¬ â†’ xb, yb = xb.to(device), yb.to(device)  
è°ƒæ¨¡åž‹ â†’ æ­£å¸¸ forward / backward
```

---

````markdown
# PyTorch å¤š GPU åŒæ—¶è®­ç»ƒæ“ä½œæŒ‡å—
> é¢å‘ **ç¬¬ä¸€æ¬¡å°è¯•å¤š GPU å¹¶è¡Œè®­ç»ƒ** çš„å®Œæ•´æ•™ç¨‹ï¼Œå¸®åŠ©ä½ ç†è§£æ¯ä¸€æ­¥åŽŸç†ï¼Œå¹¶èƒ½è½»æ¾è¿ç§»åˆ°è‡ªå·±çš„å…¶ä»–é¡¹ç›®ã€‚
---
## é¡¹ç›®ç»“æž„
```text
â”œâ”€â”€ Multi-GPU-simultaneous-training-operation-readme/   # æºç ä¸Žç¤ºä¾‹ä»£ç å®žçŽ°
â”œâ”€â”€ result_informer02_random10000GPU2/                  # å®Œæ•´æ•°æ®é›†è¿è¡Œç»“æžœå¯è§†åŒ–ï¼ˆæ—¥å¿— / æ¨¡åž‹æƒé‡ / å›¾è¡¨ï¼‰
â”œâ”€â”€ Multi-GPU-Training.png                              # å¤š GPU å·¥ä½œæ˜¾ç¤º
â”œâ”€â”€ datatry.csv                                         # ç¤ºä¾‹æ•°æ®é›†ï¼ˆ10 000 è¡Œ Ã— 12 åˆ—ï¼‰
â”œâ”€â”€ informer.py                                         # Minimal Informer è®­ç»ƒè„šæœ¬ï¼ˆå®žçŽ°å¤šGPUè®­ç»ƒinformeræ¨¡åž‹è®­ç»ƒï¼‰
â””â”€â”€ README.md                                           # æœ¬è¯´æ˜Žæ–‡ä»¶
```
---
## ç›®å½•
1. [èƒŒæ™¯çŸ¥è¯†](#èƒŒæ™¯çŸ¥è¯†)
2. [å¿«é€Ÿå¼€å§‹ï¼š5 ä¸ªå…³é”®æ­¥éª¤](#å¿«é€Ÿå¼€å§‹5-ä¸ªå…³é”®æ­¥éª¤)
3. [é€šç”¨æ¨¡æ¿](#é€šç”¨æ¨¡æ¿)
4. [å¸¸è§é”™è¯¯ä¸ŽæŽ’æŸ¥](#å¸¸è§é”™è¯¯ä¸ŽæŽ’æŸ¥)
5. [è®­ç»ƒå£è¯€](#è®­ç»ƒå£è¯€)
---
## èƒŒæ™¯çŸ¥è¯†
- åœ¨ **PyTorch** ä¸­ï¼Œæ¨¡åž‹é»˜è®¤è¿è¡Œåœ¨å•å— GPUï¼ˆå¦‚ `cuda:0`ï¼‰ã€‚
- ä¸ºäº† **åŠ é€Ÿè®­ç»ƒ**ï¼Œå¯ä»¥åŒæ—¶åˆ©ç”¨å¤šå— GPU å¤„ç†ä¸åŒæ‰¹æ¬¡æ•°æ®ã€‚
- PyTorch æä¾›ä¸¤ç§ä¸»æµæ–¹æ¡ˆï¼š  
  1. **`nn.DataParallel`**ï¼ˆå•è¿›ç¨‹å¤šçº¿ç¨‹ï¼‰â€”â€”æ˜“ä¸Šæ‰‹ï¼Œä»£ç æ”¹åŠ¨æœ€å°ï¼›å½“ GPU æ•°é‡å¾ˆå¤šæ—¶æ•ˆçŽ‡ä¸€èˆ¬ã€‚  
  2. **`torch.distributed`** / **`torchrun`**ï¼ˆå¤šè¿›ç¨‹ï¼‰â€”â€”æ‰©å±•æ€§æ›´ä½³ï¼Œé€‚åˆå¤§è§„æ¨¡é›†ç¾¤ï¼›é…ç½®æ›´å¤æ‚ã€‚
> æœ¬æ–‡ç¤ºä¾‹é‡‡ç”¨æœ€å®¹æ˜“ä¸Šæ‰‹çš„ **`nn.DataParallel`**ï¼›å¦‚éœ€æ›´é«˜æ€§èƒ½ï¼Œå¯è¿ç§»åˆ° `DistributedDataParallel`ã€‚
---
## å¿«é€Ÿå¼€å§‹ï¼š5 ä¸ªå…³é”®æ­¥éª¤  
> å‡è®¾å·²æœ‰æ¨¡åž‹ `MyModel` å’Œæ•°æ®è¿­ä»£å™¨ `train_loader` / `test_loader`ã€‚

### 1ï¸âƒ£ è®¾å®šä¸»è®¾å¤‡ `cuda:0`
```python
import torch
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
````
### 2ï¸âƒ£ å°è£…æ¨¡åž‹ä¸ºå¤š GPU æ¨¡åž‹
```python
from torch import nn
model = MyModel()
if torch.cuda.device_count() > 1:
    print(f"Using {torch.cuda.device_count()} GPUs â€¦")
    model = nn.DataParallel(model)           # ä»…éœ€è¿™ä¸€è¡Œ
model = model.to(device)
```
### 3ï¸âƒ£ **è®­ç»ƒé˜¶æ®µ**ï¼šæ•°æ®æ‰¹ä¹Ÿç§»åŠ¨åˆ°ä¸»è®¾å¤‡
```python
for xb, yb in train_loader:
    xb, yb = xb.to(device), yb.to(device)
    out = model(xb)           # DataParallel è‡ªåŠ¨åˆ‡åˆ†æ•°æ®å¹¶æ±‡æ€»æ¢¯åº¦
    # loss, backward, step â€¦
```
### 4ï¸âƒ£ **éªŒè¯ / æŽ¨ç†é˜¶æ®µ** åŒæ · `.to(device)`
```python
model.eval()
with torch.no_grad():
    for xb in test_loader:
        xb   = xb.to(device)
        pred = model(xb)
        # è¯„ä¼° â€¦
```
### 5ï¸âƒ£ ï¼ˆå¯é€‰ï¼‰æ‰“å° GPU çŠ¶æ€
```python
print(f"Available GPUs: {torch.cuda.device_count()}")
for i in range(torch.cuda.device_count()):
    print(f"GPU {i}: {torch.cuda.get_device_name(i)}")
```
---

## é€šç”¨æ¨¡æ¿

```python
import torch
from torch import nn
# ---- è®¾å¤‡ ----
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
# ---- æ¨¡åž‹ ----
model = MyModel()
if torch.cuda.device_count() > 1:
    print(f"Using {torch.cuda.device_count()} GPUs â€¦")
    model = nn.DataParallel(model)
model = model.to(device)
# ---- è®­ç»ƒå¾ªçŽ¯ ----
for xb, yb in dataloader:
    xb, yb = xb.to(device), yb.to(device)
    pred   = model(xb)
    # loss, backward, optimizer.step() â€¦
```
---

## å¸¸è§é”™è¯¯ä¸ŽæŽ’æŸ¥

| æŠ¥é”™ / çŽ°è±¡                                                                  | å¯èƒ½åŽŸå›            | å¿«é€Ÿè§£å†³                                                    |
| ------------------------------------------------------------------------ | -------------- | ------------------------------------------------------- |
| `RuntimeError: module must have its parameters on cuda:0 but got cuda:1` | ä¸»è®¾å¤‡ä¸ä¸€è‡´         | ç¡®ä¿ `device` ä¸º `cuda:0`ï¼Œå¹¶åœ¨ **å°è£…åŽ** è°ƒç”¨ `model.to(device)` |
| `Expected all tensors to be on the same device`                          | æ•°æ®æˆ–æ ‡ç­¾æœªç§»åŠ¨       | å¯¹ **æ‰€æœ‰** è¾“å…¥å’Œæ ‡ç­¾æ‰§è¡Œ `.to(device)`                          |
| å¤š GPU æœªæé€Ÿ                                                                | æ¨¡åž‹è¿‡å°æˆ– batch å¤ªå° | å¢žå¤§ `batch_size`ã€æ”¹ç”¨ `DistributedDataParallel`ï¼Œæˆ–ä»…ç”¨å•å¡      |
| `AttributeError: 'DataParallel' object has no attribute 'xxx'`           | ç›´æŽ¥è®¿é—®å°è£…åŽæ¨¡åž‹å±žæ€§    | ç”¨ `model.module.xxx` è®¿é—®å†…éƒ¨æ¨¡åž‹                             |

---

## è®­ç»ƒå£è¯€
> **å¤šå¡è®­ç»ƒäº”è¦ç´ **
>
> 1. **è®¾ä¸»å¡** â†’ `device = cuda:0`
> 2. **å°æ¨¡åž‹** â†’ `nn.DataParallel(model)`
> 3. **æ”¾ä¸»å¡** â†’ `model.to(device)`
> 4. **æ•°æ®è½¬** â†’ `xb, yb = xb.to(device), yb.to(device)`
> 5. **è°ƒæ¨¡åž‹** â†’ æ­£å¸¸ `forward / backward`

